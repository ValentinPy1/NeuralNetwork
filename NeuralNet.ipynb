{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "import os\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self, input_size, output_size, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.weights = np.random.randn(output_size, input_size)\n",
    "        self.bias = np.random.randn(output_size, 1)\n",
    "        self.m_w, self.v_w = np.zeros_like(self.weights), np.zeros_like(self.weights)\n",
    "        self.m_b, self.v_b = np.zeros_like(self.bias), np.zeros_like(self.bias)\n",
    "        self.t = 0\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return np.dot(input, self.weights.T) + self.bias.T\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        self.t += 1\n",
    "        input_gradient = np.dot(output_gradient, self.weights)\n",
    "        weights_gradient = np.dot(output_gradient.T, self.input)\n",
    "        bias_gradient = np.sum(output_gradient, axis=0, keepdims=True).T\n",
    "\n",
    "        self.m_w = self.beta1 * self.m_w + (1 - self.beta1) * weights_gradient\n",
    "        self.v_w = self.beta2 * self.v_w + (1 - self.beta2) * weights_gradient**2\n",
    "        m_w_hat = self.m_w / (1 - self.beta1**self.t)\n",
    "        v_w_hat = self.v_w / (1 - self.beta2**self.t)\n",
    "        self.weights -= learning_rate * m_w_hat / (np.sqrt(v_w_hat) + self.epsilon)\n",
    "\n",
    "        self.m_b = self.beta1 * self.m_b + (1 - self.beta1) * bias_gradient\n",
    "        self.v_b = self.beta2 * self.v_b + (1 - self.beta2) * bias_gradient**2\n",
    "        m_b_hat = self.m_b / (1 - self.beta1**self.t)\n",
    "        v_b_hat = self.v_b / (1 - self.beta2**self.t)\n",
    "        self.bias -= learning_rate * m_b_hat / (np.sqrt(v_b_hat) + self.epsilon)\n",
    "\n",
    "        return input_gradient\n",
    "\n",
    "class Dropout(Layer):\n",
    "    def __init__(self, drop_rate):\n",
    "        self.drop_rate = drop_rate\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.mask = np.random.rand(*input.shape) > self.drop_rate\n",
    "        return input * self.mask\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        return output_gradient * self.mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(Layer):\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return self.activation(input)\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        return np.multiply(output_gradient, self.activation_prime(self.input))\n",
    "\n",
    "class Tanh(Activation):\n",
    "    @staticmethod\n",
    "    def activation(x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def activation_prime(x):\n",
    "        return 1 - np.tanh(x) ** 2\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(Tanh.activation, Tanh.activation_prime)\n",
    "\n",
    "class Sigmoid(Activation):\n",
    "\n",
    "    @staticmethod\n",
    "    def activation(x):\n",
    "        x = np.clip(x, -100, 100)\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    @staticmethod\n",
    "    def activation_prime(x):\n",
    "        sig = Sigmoid.activation(x)\n",
    "        return sig * (1 - sig)\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(Sigmoid.activation, Sigmoid.activation_prime)\n",
    "\n",
    "class ReLU(Activation):\n",
    "    @staticmethod\n",
    "    def activation(x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    @staticmethod\n",
    "    def activation_prime(x):\n",
    "        return np.where(x > 0, 1, 0)\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(ReLU.activation, ReLU.activation_prime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def loss(self, y_true, y_pred):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def gradient(self, y_true, y_pred):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class MeanSquaredError(Loss):\n",
    "    def loss(self, y_true, y_pred):\n",
    "        return np.mean(np.power(y_true - y_pred, 2))\n",
    "\n",
    "    def gradient(self, y_true, y_pred):\n",
    "        return 2 * (y_pred - y_true) / y_true.size\n",
    "\n",
    "class BinaryCrossEntropy(Loss):\n",
    "    def loss(self, y_true, y_pred):\n",
    "        epsilon = 1e-15\n",
    "        return -np.mean(np.multiply(y_true, np.log(y_pred + epsilon)) + np.multiply(1 - y_true, np.log(1 - y_pred + epsilon)))\n",
    "\n",
    "    def gradient(self, y_true, y_pred):\n",
    "        epsilon = 1e-15\n",
    "        return np.divide(y_pred - y_true, np.multiply(y_pred + epsilon, 1 - y_pred + epsilon))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(Layer):\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "    \n",
    "    def forward(self, input):\n",
    "        output = input\n",
    "        for layer in self.layers:\n",
    "            output = layer.forward(output)\n",
    "        return output\n",
    "    \n",
    "    def backward(self, output_error, learning_rate):\n",
    "        for layer in reversed(self.layers):\n",
    "            output_error = layer.backward(output_error, learning_rate)\n",
    "        return output_error\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, network, loss):\n",
    "        self.network = Network(network)\n",
    "        self.loss = loss\n",
    "        self.train_loss = []\n",
    "        self.test_loss = []\n",
    "        self.train_acc = []\n",
    "        self.test_acc = []\n",
    "    \n",
    "    def train(self, x_train, y_train, x_test, y_test, epochs, batch_size=64, learning_rate=0.001, early_stop=0, verbose=True, progress_bar=False):\n",
    "        test_count = x_test.shape[0]\n",
    "        for epoch in range(epochs):\n",
    "            perm = np.random.permutation(len(x_train))\n",
    "            x_train = x_train[perm]\n",
    "            y_train = y_train[perm]\n",
    "            mean_loss = 0\n",
    "            for i in tqdm(range(0, len(x_train), batch_size), disable=not progress_bar):\n",
    "                x_batch = x_train[i:i+batch_size]\n",
    "                y_batch = y_train[i:i+batch_size]\n",
    "                output = self.network.forward(x_batch)\n",
    "                loss = self.loss.loss(y_batch, output)\n",
    "                mean_loss += loss\n",
    "                loss_gradient = self.loss.gradient(y_batch, output)\n",
    "                self.network.backward(loss_gradient, learning_rate)\n",
    "                if early_stop > 0 and len(self.test_acc) > early_stop and self.test_acc[-1] <= self.test_acc[-early_stop]:\n",
    "                    return\n",
    "            mean_loss /= len(x_train) / batch_size\n",
    "            self.train_loss.append(mean_loss)\n",
    "            train_accu = self.test_accuracy(x_train[:test_count], y_train[:test_count])\n",
    "            self.train_acc.append(train_accu)\n",
    "            self.evaluate(x_test, y_test)\n",
    "            if verbose:\n",
    "                print(f\"Epoch {epoch + 1} - Train loss: {round(self.train_loss[-1], 4)} - Test loss: {round(self.test_loss[-1], 4)} - Train acc: {round(self.train_acc[-1], 4)} - Test acc: {round(self.test_acc[-1], 4)}\")\n",
    "    \n",
    "    def evaluate(self, x_test, y_test):\n",
    "        total_loss = 0\n",
    "        correct_predictions = 0\n",
    "        for x, y in zip(x_test, y_test):\n",
    "            output = self.network.forward(x)\n",
    "            total_loss += self.loss.loss(y, output)\n",
    "            if np.argmax(output) == np.argmax(y):\n",
    "                correct_predictions += 1\n",
    "        avg_loss = total_loss / len(x_test)\n",
    "        accuracy = correct_predictions / len(x_test)\n",
    "        self.test_loss.append(avg_loss)\n",
    "        self.test_acc.append(accuracy)\n",
    "\n",
    "    def predict_one(self, x):\n",
    "        return self.network.forward(x)\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        y_pred = []\n",
    "        for x in x_test:\n",
    "            y_pred.append(self.predict_one(x))\n",
    "        return np.array(y_pred)\n",
    "\n",
    "    def test_accuracy(self, x_test, y_test):\n",
    "        correct_predictions = 0\n",
    "        for x, y in zip(x_test, y_test):\n",
    "            output = self.network.forward(x)\n",
    "            if np.argmax(output) == np.argmax(y):\n",
    "                correct_predictions += 1\n",
    "        accuracy = correct_predictions / len(x_test)\n",
    "        return accuracy        \n",
    "\n",
    "    def save(self, path):\n",
    "        with open(path, \"wb\") as f:\n",
    "            pickle.dump(self, f)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path):\n",
    "        with open(path, \"rb\") as f:\n",
    "            return pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Preprocessing data\n",
      "One hot encoding\n",
      "Flattening\n",
      "Reshaping\n",
      "Shuffling\n",
      "X train shape: (239221, 897)\n",
      "Y train shape: (239221, 4)\n",
      "X test shape: (12591, 897)\n",
      "Y test shape: (12591, 4)\n"
     ]
    }
   ],
   "source": [
    "from Sources.DataLoader import load_chess_data\n",
    "\n",
    "def one_hot_encode(x):\n",
    "    one_hot = np.zeros((x.shape[0], 69, 13))\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(69):\n",
    "            one_hot[i][j][x[i][j]] = 1\n",
    "    return one_hot\n",
    "\n",
    "def flatten(x):\n",
    "    return x.reshape(x.shape[0], x.shape[1] * x.shape[2])\n",
    "\n",
    "def preprocess_data(x, y):\n",
    "    print(\"One hot encoding\")\n",
    "    x = one_hot_encode(x)\n",
    "    print(\"Flattening\")\n",
    "    x = flatten(x)\n",
    "    print(\"Reshaping\")\n",
    "    x = x.reshape(x.shape[0], x.shape[1])\n",
    "    y = y.reshape(y.shape[0], y.shape[1])\n",
    "    print(\"Shuffling\")\n",
    "    perm = np.random.permutation(len(x))\n",
    "    x = x[perm]\n",
    "    y = y[perm]\n",
    "    return x, y\n",
    "\n",
    "def split_data(x, y, split):\n",
    "    split_index = int(split * len(x))\n",
    "    x_train, x_test = x[:split_index], x[split_index:]\n",
    "    y_train, y_test = y[:split_index], y[split_index:]\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "print(\"Loading data\")\n",
    "x, y = load_chess_data(\"datasets/equiComb.txt\")\n",
    "print(\"Preprocessing data\")\n",
    "x, y = preprocess_data(x, y)\n",
    "x_train, y_train, x_test, y_test = split_data(x, y, 0.95)\n",
    "x, y = None, None\n",
    "print(\"X train shape:\", x_train.shape)\n",
    "print(\"Y train shape:\", y_train.shape)\n",
    "print(\"X test shape:\", x_test.shape)\n",
    "print(\"Y test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23778889683106982\n",
      "Epoch 1 - Train loss: 0.7858 - Test loss: 0.4963 - Train acc: 0.4668 - Test acc: 0.4673\n",
      "Epoch 2 - Train loss: 0.4794 - Test loss: 0.4667 - Train acc: 0.5181 - Test acc: 0.5146\n",
      "Epoch 3 - Train loss: 0.4557 - Test loss: 0.4488 - Train acc: 0.5285 - Test acc: 0.5387\n",
      "Epoch 4 - Train loss: 0.4423 - Test loss: 0.44 - Train acc: 0.5495 - Test acc: 0.5491\n",
      "Epoch 5 - Train loss: 0.4314 - Test loss: 0.4287 - Train acc: 0.5491 - Test acc: 0.5552\n",
      "Epoch 6 - Train loss: 0.4236 - Test loss: 0.4199 - Train acc: 0.5594 - Test acc: 0.5622\n",
      "Epoch 7 - Train loss: 0.4158 - Test loss: 0.4163 - Train acc: 0.5745 - Test acc: 0.5683\n",
      "Epoch 8 - Train loss: 0.4083 - Test loss: 0.4078 - Train acc: 0.5843 - Test acc: 0.5781\n",
      "Epoch 9 - Train loss: 0.402 - Test loss: 0.403 - Train acc: 0.585 - Test acc: 0.5784\n",
      "Epoch 10 - Train loss: 0.3958 - Test loss: 0.3951 - Train acc: 0.5975 - Test acc: 0.5914\n"
     ]
    }
   ],
   "source": [
    "layers = [\n",
    "    Dense(897, 400),\n",
    "    Tanh(),\n",
    "    Dropout(0.5),\n",
    "    Dense(400, 64),\n",
    "    Tanh(),\n",
    "    Dense(64, 4),\n",
    "    Sigmoid()\n",
    "]\n",
    "\n",
    "model = Model(layers, BinaryCrossEntropy())\n",
    "print(model.test_accuracy(x_test, y_test))\n",
    "model.train(x_train, y_train, x_test, y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/69_400_400_64_4_equiComb.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model.load(\"my_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43602573266619016"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.test_accuracy(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fig, ax1 \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39msubplots()\n\u001b[1;32m      3\u001b[0m ax1\u001b[38;5;241m.\u001b[39mset_xlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m ax1\u001b[38;5;241m.\u001b[39mset_ylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.plot(model.train_loss, color='tab:blue', linestyle='dashed')\n",
    "ax1.plot(model.test_loss, color='tab:orange')\n",
    "ax1.tick_params(axis='y')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.plot(model.train_acc, color='tab:blue', linestyle='dashed')\n",
    "ax2.plot(model.test_acc, color='tab:orange')\n",
    "ax2.tick_params(axis='y')\n",
    "\n",
    "plt.legend([\"Train\", \"Test\", \"Train loss\", \"Test loss\"], loc='upper left')\n",
    "\n",
    "plt.title(\"Model accuracy and loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models/model_200_dropout_100'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels/model_200_dropout_100\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 90\u001b[0m, in \u001b[0;36mModel.load\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(path):\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mload(f)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/IPython/core/interactiveshell.py:308\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    304\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m     )\n\u001b[0;32m--> 308\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models/model_200_dropout_100'"
     ]
    }
   ],
   "source": [
    "model = Model.load(\"models/model_200_dropout_100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: Checkmate\n",
      "Reality: 3237\n",
      "Correct predictions: 1596\n",
      "Total Predictions: 3030\n",
      "Accuracy: 0.5267\n",
      "\n",
      "Class: Check\n",
      "Reality: 3113\n",
      "Correct predictions: 1439\n",
      "Total Predictions: 2820\n",
      "Accuracy: 0.5103\n",
      "\n",
      "Class: Stalemate\n",
      "Reality: 3194\n",
      "Correct predictions: 2875\n",
      "Total Predictions: 3676\n",
      "Accuracy: 0.7821\n",
      "\n",
      "Class: Nothing\n",
      "Reality: 3047\n",
      "Correct predictions: 1738\n",
      "Total Predictions: 3065\n",
      "Accuracy: 0.567\n",
      "\n",
      "Total accuracy: 0.6074\n"
     ]
    }
   ],
   "source": [
    "def cross_test(model, x_test, y_test):\n",
    "    pred_count = [0, 0, 0, 0]\n",
    "    correct_pred_count = [0, 0, 0, 0]\n",
    "    reality_count = [0, 0, 0, 0]\n",
    "    for x, y in zip(x_test, y_test):\n",
    "        pred = np.argmax(model.predict_one(x))\n",
    "        reality = np.argmax(y)\n",
    "        pred_count[pred] += 1\n",
    "        reality_count[reality] += 1\n",
    "        if pred == reality:\n",
    "            correct_pred_count[pred] += 1\n",
    "    classnames = [\"Checkmate\", \"Check\", \"Stalemate\", \"Nothing\"]\n",
    "    for i in range(4):\n",
    "        print(f\"Class: {classnames[i]}\")\n",
    "        print(f\"Reality: {reality_count[i]}\")\n",
    "        print(f\"Correct predictions: {correct_pred_count[i]}\")\n",
    "        print(f\"Total Predictions: {pred_count[i]}\")\n",
    "        print(f\"Accuracy: {round(correct_pred_count[i] / pred_count[i], 4)}\")\n",
    "        print()\n",
    "    print(f\"Total accuracy: {round(sum(correct_pred_count) / sum(pred_count), 4)}\")\n",
    "cross_test(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaping\n",
      "Shuffling\n",
      "x shape: (188120, 69)\n",
      "y shape: (188120, 4)\n"
     ]
    }
   ],
   "source": [
    "x, y = load_chess_data(\"datasets/clement.txt\")\n",
    "x, y = preprocess_data(x, y)\n",
    "\n",
    "print(\"x shape:\", x.shape)\n",
    "print(\"y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: Checkmate\n",
      "Reality: 47030\n",
      "Correct predictions: 25959\n",
      "Total Predictions: 51645\n",
      "Accuracy: 0.5026\n",
      "\n",
      "Class: Check\n",
      "Reality: 47030\n",
      "Correct predictions: 15633\n",
      "Total Predictions: 34818\n",
      "Accuracy: 0.449\n",
      "\n",
      "Class: Stalemate\n",
      "Reality: 47030\n",
      "Correct predictions: 44294\n",
      "Total Predictions: 62021\n",
      "Accuracy: 0.7142\n",
      "\n",
      "Class: Nothing\n",
      "Reality: 47030\n",
      "Correct predictions: 21667\n",
      "Total Predictions: 39636\n",
      "Accuracy: 0.5466\n",
      "\n",
      "Total accuracy: 0.5717\n"
     ]
    }
   ],
   "source": [
    "cross_test(model, x, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
